{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from collections.abc import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import load_model\n",
    "from models.get_model import get_model\n",
    "\n",
    "from monai.networks.blocks.segresnet_block import ResBlock, get_conv_layer, get_upsample_layer\n",
    "from monai.networks.layers.factories import Dropout\n",
    "from monai.networks.layers.utils import get_act_layer, get_norm_layer\n",
    "from monai.utils import UpsampleMode\n",
    "from typing import Union, Tuple, List, Dict, Optional\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # RBG to grayscale\n",
    "        x = torch.mean(x, dim=-3, keepdim=True)\n",
    "        # add batch dim\n",
    "        # x = x.unsqueeze(0)\n",
    "\n",
    "        x = self.model(x)\n",
    "        # do argmax\n",
    "        x = torch.softmax(x, dim=1)\n",
    "        x = x[:, 1, ...]\n",
    "        x = x.unsqueeze(1)\n",
    "        x = x > 0.9\n",
    "\n",
    "        res : Dict[str, torch.Tensor] = {}\n",
    "        res[\"out\"] = x\n",
    "        # res = x\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"2023-11-18_16-23-20\"\n",
    "\n",
    "run_path = f\"runs/{run_name}/\"\n",
    "\n",
    "train_summary = json.load(open(run_path + \"train_summary.json\"))\n",
    "\n",
    "model_name = train_summary[\"config\"][\"MODEL\"]\n",
    "IMAGE_SIZE = train_summary[\"config\"][\"IMAGE_SIZE\"]\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "model = get_model(model_name, IMAGE_SIZE)\n",
    "model = load_model(model, run_path + \"best_model.pth\")\n",
    "\n",
    "model_mobile = MobileWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "model_mobile.eval()\n",
    "example = torch.rand(1, 3, 256, 256)\n",
    "\n",
    "out = model_mobile(example)\n",
    "print(out[\"out\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mobile.eval()\n",
    "example = torch.rand(1, 3, 256, 256)\n",
    "# traced_module = torch.jit.trace(model, example, strict=False)\n",
    "traced_script_module = torch.jit.script(model_mobile)\n",
    "traced_script_module_optimized = optimize_for_mobile(traced_script_module)\n",
    "traced_script_module_optimized._save_for_lite_interpreter(\"model.ptl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "from torch.jit.mobile import (\n",
    "    _backport_for_mobile,\n",
    "    _get_model_bytecode_version,\n",
    ")\n",
    "\n",
    "print(_get_model_bytecode_version(\"model.ptl\"))\n",
    "\n",
    "_backport_for_mobile(\"model.ptl\", \"model_7.ptl\", 7)\n",
    "\n",
    "print(_get_model_bytecode_version(\"model_7.ptl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/rikhat.akizhanov/.cache/torch/hub/pytorch_vision_v0.11.0\n",
      "/home/rikhat.akizhanov/Desktop/old_torch/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rikhat.akizhanov/Desktop/old_torch/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.11.0', 'deeplabv3_resnet50', pretrained=True)\n",
    "model.eval();\n",
    "\n",
    "scripted_module = torch.jit.script(model)\n",
    "optimized_scripted_module = optimize_for_mobile(scripted_module)\n",
    "\n",
    "# Export full jit version model (not compatible with lite interpreter)\n",
    "scripted_module.save(\"deeplabv3_scripted.pt\")\n",
    "# Export lite interpreter version model (compatible with lite interpreter)\n",
    "scripted_module._save_for_lite_interpreter(\"deeplabv3_scripted.ptl\")\n",
    "# using optimized lite interpreter model makes inference about 60% faster than the non-optimized lite interpreter model, which is about 6% faster than the non-optimized full jit model\n",
    "optimized_scripted_module._save_for_lite_interpreter(\"deeplabv3_scripted_optimized.ptl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 21, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "img = torch.rand(1, 3, 256, 256)\n",
    "out = model(img)\n",
    "\n",
    "print(out[\"out\"].shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV701_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

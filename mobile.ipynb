{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from collections.abc import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import load_model\n",
    "\n",
    "from monai.networks.blocks.segresnet_block import ResBlock, get_conv_layer, get_upsample_layer\n",
    "from monai.networks.layers.factories import Dropout\n",
    "from monai.networks.layers.utils import get_act_layer, get_norm_layer\n",
    "from monai.utils import UpsampleMode\n",
    "from typing import Union, Tuple, List, Dict, Optional\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims: int = 3,\n",
    "        init_filters: int = 8,\n",
    "        in_channels: int = 1,\n",
    "        out_channels: int = 2,\n",
    "        dropout_prob: Union[float, None] = None,\n",
    "        act: Union[Tuple, str] = (\"RELU\", {\"inplace\": True}),\n",
    "        norm: Union[Tuple, str] = (\"GROUP\", {\"num_groups\": 8}),\n",
    "        norm_name: str = \"\",\n",
    "        num_groups: int = 8,\n",
    "        use_conv_final: bool = True,\n",
    "        blocks_down: Tuple[int, ...] = (1, 2, 2, 4),\n",
    "        blocks_up: Tuple[int, ...] = (1, 1, 1),\n",
    "        upsample_mode: Union[UpsampleMode, str] = UpsampleMode.NONTRAINABLE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if spatial_dims not in (2, 3):\n",
    "            raise ValueError(\"`spatial_dims` can only be 2 or 3.\")\n",
    "\n",
    "        self.spatial_dims = spatial_dims\n",
    "        self.init_filters = init_filters\n",
    "        self.in_channels = in_channels\n",
    "        self.blocks_down = blocks_down\n",
    "        self.blocks_up = blocks_up\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.act = act  # input options\n",
    "        self.act_mod = get_act_layer(act)\n",
    "        if norm_name:\n",
    "            if norm_name.lower() != \"group\":\n",
    "                raise ValueError(f\"Deprecating option 'norm_name={norm_name}', please use 'norm' instead.\")\n",
    "            norm = (\"group\", {\"num_groups\": num_groups})\n",
    "        self.norm = norm\n",
    "        self.upsample_mode = UpsampleMode(upsample_mode)\n",
    "        self.use_conv_final = use_conv_final\n",
    "        self.convInit = get_conv_layer(spatial_dims, in_channels, init_filters)\n",
    "        self.down_layers = self._make_down_layers()\n",
    "        self.up_layers, self.up_samples = self._make_up_layers()\n",
    "        self.conv_final = self._make_final_conv(out_channels)\n",
    "\n",
    "        if dropout_prob is not None:\n",
    "            self.dropout = Dropout[Dropout.DROPOUT, spatial_dims](dropout_prob)\n",
    "\n",
    "    def _make_down_layers(self):\n",
    "        down_layers = nn.ModuleList()\n",
    "        blocks_down, spatial_dims, filters, norm = (self.blocks_down, self.spatial_dims, self.init_filters, self.norm)\n",
    "        for i, item in enumerate(blocks_down):\n",
    "            layer_in_channels = filters * 2**i\n",
    "            pre_conv = (\n",
    "                get_conv_layer(spatial_dims, layer_in_channels // 2, layer_in_channels, stride=2)\n",
    "                if i > 0\n",
    "                else nn.Identity()\n",
    "            )\n",
    "            down_layer = nn.Sequential(\n",
    "                pre_conv, *[ResBlock(spatial_dims, layer_in_channels, norm=norm, act=self.act) for _ in range(item)]\n",
    "            )\n",
    "            down_layers.append(down_layer)\n",
    "        return down_layers\n",
    "\n",
    "    def _make_up_layers(self):\n",
    "        up_layers, up_samples = nn.ModuleList(), nn.ModuleList()\n",
    "        upsample_mode, blocks_up, spatial_dims, filters, norm = (\n",
    "            self.upsample_mode,\n",
    "            self.blocks_up,\n",
    "            self.spatial_dims,\n",
    "            self.init_filters,\n",
    "            self.norm,\n",
    "        )\n",
    "        n_up = len(blocks_up)\n",
    "        for i in range(n_up):\n",
    "            sample_in_channels = filters * 2 ** (n_up - i)\n",
    "            up_layers.append(\n",
    "                nn.Sequential(\n",
    "                    *[\n",
    "                        ResBlock(spatial_dims, sample_in_channels // 2, norm=norm, act=self.act)\n",
    "                        for _ in range(blocks_up[i])\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            up_samples.append(\n",
    "                nn.Sequential(\n",
    "                    *[\n",
    "                        get_conv_layer(spatial_dims, sample_in_channels, sample_in_channels // 2, kernel_size=1),\n",
    "                        get_upsample_layer(spatial_dims, sample_in_channels // 2, upsample_mode=upsample_mode),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        return up_layers, up_samples\n",
    "\n",
    "    def _make_final_conv(self, out_channels: int):\n",
    "        return nn.Sequential(\n",
    "            get_norm_layer(name=self.norm, spatial_dims=self.spatial_dims, channels=self.init_filters),\n",
    "            self.act_mod,\n",
    "            get_conv_layer(self.spatial_dims, self.init_filters, out_channels, kernel_size=1, bias=True),\n",
    "        )\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n",
    "        x = self.convInit(x)\n",
    "        if self.dropout_prob is not None:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        down_x = []\n",
    "\n",
    "        for down in self.down_layers:\n",
    "            x = down(x)\n",
    "            down_x.append(x)\n",
    "\n",
    "        return x, down_x\n",
    "\n",
    "    def decode(self, x: torch.Tensor, down_x: List[torch.Tensor]) -> torch.Tensor:\n",
    "        for i, (up, upl) in enumerate(zip(self.up_samples, self.up_layers)):\n",
    "            x = up(x) + down_x[i + 1]\n",
    "            x = upl(x)\n",
    "\n",
    "        if self.use_conv_final:\n",
    "            x = self.conv_final(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x, down_x = self.encode(x)\n",
    "        down_x.reverse()\n",
    "\n",
    "        x = self.decode(x, down_x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class MobileWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # RBG to grayscale\n",
    "        x = torch.mean(x, dim=1, keepdim=True)\n",
    "        # add batch dim\n",
    "        # x = x.unsqueeze(0)\n",
    "\n",
    "        x = self.model(x)\n",
    "\n",
    "        res : Dict[str, torch.Tensor] = {}\n",
    "        res[\"out\"] = x\n",
    "        return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"2023-11-08_19-49-52\"\n",
    "\n",
    "run_path = f\"runs/{run_name}/\"\n",
    "\n",
    "train_summary = json.load(open(run_path + \"train_summary.json\"))\n",
    "\n",
    "model_name = train_summary[\"config\"][\"MODEL\"]\n",
    "IMAGE_SIZE = train_summary[\"config\"][\"IMAGE_SIZE\"]\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "\n",
    "\n",
    "model = SegResNet(in_channels=1, out_channels=2, spatial_dims=2)\n",
    "model = load_model(model, run_path + \"best_model.pth\")\n",
    "\n",
    "model_mobile = MobileWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'out': tensor([[[[ 1.0446,  1.0462,  0.9783,  ...,  0.9745,  0.9883,  1.0939],\n",
      "          [ 1.0527,  1.1909,  1.1640,  ...,  1.0258,  1.0346,  1.0487],\n",
      "          [ 1.3599,  1.2263,  1.4739,  ...,  1.0556,  0.9517,  0.9987],\n",
      "          ...,\n",
      "          [ 1.9674,  2.0573,  1.8549,  ...,  1.7608,  1.6719,  0.8930],\n",
      "          [ 2.0589,  2.0807,  2.1081,  ...,  1.9305,  1.7167,  0.8926],\n",
      "          [ 1.7014,  1.6551,  1.6030,  ...,  1.6165,  1.2686,  0.8988]],\n",
      "\n",
      "         [[-3.2444, -3.2493, -3.0274,  ..., -3.0148, -3.0919, -3.4127],\n",
      "          [-3.2707, -3.2028, -2.7732,  ..., -3.1828, -3.2115, -3.2644],\n",
      "          [-3.1608, -2.7303, -2.8102,  ..., -3.2803, -2.9402, -3.0942],\n",
      "          ...,\n",
      "          [-3.1360, -3.0108, -2.5942,  ..., -2.6833, -3.0548, -2.7484],\n",
      "          [-3.6295, -3.1460, -3.2558,  ..., -2.9810, -3.2235, -2.7470],\n",
      "          [-3.4532, -3.3819, -3.1156,  ..., -3.1775, -3.0324, -2.7674]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "model_mobile.eval()\n",
    "example = torch.rand(1, 3, 256, 256)\n",
    "\n",
    "out = model_mobile(example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mobile.eval()\n",
    "example = torch.rand(1, 3, 256, 256)\n",
    "# traced_module = torch.jit.trace(model, example, strict=False)\n",
    "traced_script_module = torch.jit.script(model_mobile)\n",
    "traced_script_module_optimized = optimize_for_mobile(traced_script_module)\n",
    "traced_script_module_optimized._save_for_lite_interpreter(\"model.ptl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "from torch.jit.mobile import (\n",
    "    _backport_for_mobile,\n",
    "    _get_model_bytecode_version,\n",
    ")\n",
    "\n",
    "print(_get_model_bytecode_version(\"model.ptl\"))\n",
    "\n",
    "_backport_for_mobile(\"model.ptl\", \"model_7.ptl\", 7)\n",
    "\n",
    "print(_get_model_bytecode_version(\"model_7.ptl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/rikhat.akizhanov/.cache/torch/hub/pytorch_vision_v0.11.0\n",
      "/home/rikhat.akizhanov/Desktop/old_torch/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rikhat.akizhanov/Desktop/old_torch/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.11.0', 'deeplabv3_resnet50', pretrained=True)\n",
    "model.eval();\n",
    "\n",
    "scripted_module = torch.jit.script(model)\n",
    "optimized_scripted_module = optimize_for_mobile(scripted_module)\n",
    "\n",
    "# Export full jit version model (not compatible with lite interpreter)\n",
    "scripted_module.save(\"deeplabv3_scripted.pt\")\n",
    "# Export lite interpreter version model (compatible with lite interpreter)\n",
    "scripted_module._save_for_lite_interpreter(\"deeplabv3_scripted.ptl\")\n",
    "# using optimized lite interpreter model makes inference about 60% faster than the non-optimized lite interpreter model, which is about 6% faster than the non-optimized full jit model\n",
    "optimized_scripted_module._save_for_lite_interpreter(\"deeplabv3_scripted_optimized.ptl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 21, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "img = torch.rand(1, 3, 256, 256)\n",
    "out = model(img)\n",
    "\n",
    "print(out[\"out\"].shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV701_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
